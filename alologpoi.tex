\documentclass{article}
\title{ALO for Logistic Regression and Poisson Regression}
\author{Yuze Zhou}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{graphicx}
\begin{document}
\section{ALO for Logistic Regression}
\subsection{ALO for Logistic Regression with Lasso penalty}
\paragraph{}First Let's rewrite the optimization problem with the loss functions separated for each observation, therefore the loss function goes:
\begin{center}
$-\sum\limits_{i}(y_{i}x_{i}^{\tau}\beta+log(1+exp(x_{i}^{\tau})))+\lambda_{1}||\beta||_{1}$
\end{center}
\paragraph{}Where, separatively, the loss function is $l(x_{i}^{\tau}\beta ;y_{i}) = y_{i}x_{i}^{\tau}\beta+log(1+exp(x_{i}^{\tau}\beta))$ and the regularizer is $R(\beta) = \lambda_{1}||\beta||_{1}$, from which we could derive the dual optimal $\hat{\theta} = y - \frac{e^{X\beta}}{1+e^{X\beta}}$, as well as the conjugate functions of them as $l^{*}(-\theta_{i};y_{i})=(y_{i}-\theta_{i})ln\frac{y_{i}-\theta_{i}}{1-(y_{i}-\theta_{i})}-ln\frac{1}{1-(y_{i}-\theta_{i})}$,
\begin{center}
$R^{*}(\beta) = \left\{
\begin{aligned}
0 \quad ||\beta||_{\infty} \leq \lambda_{1}\\
\infty \quad o.w.\\
\end{aligned}
\right.$
\end{center}
\paragraph{}From the results of the conjugate functions above, we could also obtain the derivatives of the loss functions and the Jacobian of the regularizer:
\begin{center}
$\dot{l}^{*}(-\theta_{i};y_{i}) = ln \frac{y_{i}-\theta_{i}}{1-(y_{i}-\theta_{i})}$\\
$\ddot{l}^{*}(-\theta_{i};y_{i}) = \frac{1}{(y_{i}-\theta_{i})(1-(y_{i}-\theta_{i}))}$
\end{center}
\paragraph{}Recall (15) from the \textbf{General Smooth Loss}, the quadratic surrogate of the dual problem is $\min\limits_{u} \frac{1}{2}\sum\limits_{i}(u_{i}-\frac{\hat{\theta}_{i}\ddot{l}^{*}(-\hat{\theta}_{i};y_{i})+\hat{y}_{i}}{\sqrt{\ddot{l}^{*}(-\hat{\theta}_{i};y_{i})}})^{2}+R^{*}(X^{\tau}Ku)$, where $K =diag(\sqrt{\ddot{l}^{*}(-\hat{\theta}_{i};y_{i})})$ therefore the Jacobian at $y_{u} = \frac{\hat{\theta}_{i}\ddot{l}^{*}(-\hat{\theta}_{i};y_{i})+\hat{\theta}_{i}}{\sqrt{\ddot{l}^{*}(-\hat{\theta}_{i};y_{i})}}$ could locally be treated as the projection onto the orthogonal complement of the polyhedra $\{||X^{\tau}Ku||_{\infty} \leq \lambda_{1}\}$, thus $J = I - X_{u,E}(X_{u,E}^{\tau}X_{u,E})^{-1}X_{u,E}$, where $X_{u,E}$ are the columns of $X_{u}=X^{\tau}K$, such that the columns in the set $E = \{|X_{i}^{\tau}\theta| = \lambda_{1}\}$ are selected. Take everything to (17), $y^{/i} = K_{ii}(y_{u,i}-\frac{K_{ii}\hat{\theta}_{i}}{J_{ii}})$, we could obtain the alo for the $i$th observation.
\subsection{ALO for Logistic Regression with Elastic Net Penalty}
\paragraph{}The optimization problem for logistic regression with elastic net penalty is:
\begin{center}
$-\sum\limits_{i}(y_{i}x_{i}^{\tau}\beta+log(1+exp(x_{i}^{\tau})))+\lambda_{1}||\beta||_{1}+\lambda_{2}||\beta||_{2}^{2}$
\end{center}
\paragraph{}The optimization problem is the same except the regularizer is changed, therefore the only thing different is the conjugate function of the regularizer, $R^{*}$ and the corresponding Jacobian, here $R(\beta) = \lambda_{1}||\beta||_{1}+\lambda_{2}||\beta||_{2}^{2}$:
\begin{center}
$R^{*}(\beta) = \sum\limits_{|u_{i}| > \lambda_{1}} \frac{(\lambda_{1}-|u_{i}|)^{2}}{4\lambda_{2}}$
\end{center}
\paragraph{}The corresponding Jacobian is $J = (I + \frac{1}{2\lambda_{2}}X_{u,E}X_{u,E}^{\tau})$, where $X_{u,E}$ are the columns of $X_{u}=X^{\tau}K$, such that the columns in the set $E = \{|X_{i}^{\tau}\theta| = \lambda_{1}\}$ are selected. Take everything to (17), $y^{/i} = K_{ii}(y_{u,i}-\frac{K_{ii}\hat{\theta}_{i}}{J_{ii}})$, we could obtain the alo for the $i$th observation.
\section{ALO for Poisson Regression}
\subsection{ALO for Poisson Regression with Lasso Penalty}
\paragraph{}The optimization function for Poisson regression with lasso penalty is:
\begin{center}
$\sum\limits_{i}-y_{i}x_{i}^{\tau}\beta+e^{x_{i}^{\tau}\beta}+log(y_{i}!) + \lambda_{1}||\beta||_{1}$
\end{center}
\paragraph{}The regularizer is the same as the logistic regression with the lasso penalty case, thus the Jacobian will also be the same, therefore we only have to focus on the loss function $l(x_{i}^{\tau}\beta ; y_{i}) = -y_{i}x_{i}^{\tau}\beta+e^{x_{i}^{\tau}\beta}+log(y_{i}!)$. The optimal solution for the dual problem $\hat{\theta} = y - e^{X\beta}$ and the conjugate of the loss function is $l^{*}(-\theta_{i};y_{i}) = (y_{i}-\theta_{i})ln(y_{i}-\theta_{i})-(y_{i}-\theta_{i})$, the corresponding derivatives are therefore:
\begin{center}
$\dot{l}^{*}(-\theta_{i};y_{i}) = ln(y_{i}-\theta_{i})$\\
$\ddot{l}^{*}(-\theta_{i};y_{i}) = \frac{1}{y_{i}-\theta_{i}}$
\end{center}
\paragraph{}By plugging everything into (17), we obtain the alo for Poisson regression with the lasso penalty.
\subsection{ALO for Poisson Regression with Elastic Net Penalty}
\paragraph{}The loss function for Poisson regression with elastic net penalty is the same as that of Poisson regression with the lasso penalty and the regularizer of it is the same as that of logistic regression with elastic net penalty, thus by plugging everything into (17), we could obtain the alo for Poisson regression with elastic net penalty.
\end{document}
